### 知识架构

- 损失函数是什么？
  - 衡量模型预测结果与真实结果之间差异或误差的**函数**。
- 常见的损失函数有哪些？
  - 此处详细介绍交叉熵损失函数和`softmax`损失函数。
- 如何利用损失函数？
  - 运用优化算法对模型参数进行调整，以改变预测结果，通过对预测结果的改变来让损失函数达到最小化，即让预测结果尽量接近真实结果。 
- 常见的优化算法有哪些？
  - 此处详细介绍梯度下降算法。

### 损失函数

- 损失函数（Loss Function）是机器学习和深度学习中用于衡量模型预测结果与真实结果之间差异或误差的**函数**。
- 它是一个数值评估指标，通过对模型输出和真实标签之间的比较，提供了对模型性能的度量。损失函数的值越小，表示模型的预测结果与真实结果越接近，模型的性能也就越好。

### 交叉熵

- **定义**：交叉熵用于衡量一个概率分布 $P$ 与另一个概率分布 $Q$ 之间的差异。具体来说，它衡量的是用概率分布 $Q$ 来表示概率分布 $P$ 所需的平均信息量。交叉熵的数学表达式为：

  $H(P,Q)=−x∑P(x)logQ(x)$

  其中：

  - $P(x)$ 是真实分布，表示事件 *x* 发生的真实概率。
  - $Q(x)$ 是预测分布，表示事件 *x* 发生的预测概率。
  - $log$ 通常以$2$为底或以自然对数 $e$ 为底，具体取决于应用场景。

- **概率分布**：

  - 是描述一个随机变量所有可能取值及其对应概率的函数。对于离散型随机变量，通过概率质量函数（PMF）表示每个取值的概率；对于连续型随机变量，则通过概率密度函数（PDF）表示在某个区间内取值的概率。
  - 所有概率分布都必须满足非负性、归一化和完备性，并可以通过累积分布函数（CDF）来描述。

- **信息量**：

  - 信息量是衡量一个事件发生时所携带的信息量的指标。
  - 信息量的定义为：$I(x)=−logP(x)$
    - 其中 $P(x)$ 是事件 $x$ 发生的概率。信息量越大，表示事件 *x* 发生时所携带的信息越多。

- **期望信息量（熵）**：

  - 期望信息量是所有可能事件的信息量的加权平均值，权重是事件发生的概率。对于真实分布 *P*，期望信息量为：

    $H(P)=−x∑P(x)logP(x)$

  - 这就是熵（Entropy）的定义，表示真实分布 $P$ 的不确定性.

### 交叉熵损失函数

- **定义**：
  
  - 交叉熵损失函数（Cross-Entropy Loss）是一种用于衡量两个概率分布之间差异的损失函数。
  
  - 交叉熵损失函数的目标是通过**最小化**预测概率分布与真实标签分布之间的**差异**，来**优化**模型的**参数**。
  
    
  
- **应用**：它在机器学习和深度学习中广泛应用于分类问题，特别是在多分类任务中。

#### 交叉熵损失的定义
1. **一般形式**：
$$H(p,q)=-\sum_{x}p(x)\log q(x)$$
2. **在分类问题中的简化形式（独热编码情况）**：
$$H(p,q)=-\sum_{i}p_{i}\log q_{i}$$
3. **进一步简化形式（独热编码中实际类别情况）**：
$$H(p,q)=-\log q_{\text{实际类别}}$$

#### 多个样本的平均损失
$$L = -\frac{1}{N}\sum_{i = 1}^{N}\log q_{i,y_{i}}$$

#### 概率剪辑
$$q_{\text{剪辑}}=\max(\epsilon,\min(1-\epsilon,q))$$

#### KL散度
$$KL(p\parallel q)=H(p,q)-H(p)$$

#### 二元分类中的二元交叉熵损失
$$L = -\frac{1}{N}\sum_{i = 1}^{N}[y_{i}\log(p_{i})+(1 - y_{i})\log(1 - p_{i})]$$ 

### `Softmax`函数

- **定义**：
  - `Softmax` 函数将模型的输出 $z$ 转换为概率分布 $p$.
  - $$\text{Softmax}(z_k)=\frac{\exp(z_k)}{\sum_{j = 1}^{K}\exp(z_j)}$$.

### `Softmax`损失函数

- **定义**：
  - `Softmax` 损失函数的核心思想是将模型的输出转换为概率分布，然后通过交叉熵损失函数衡量这个概率分布与真实标签分布之间的差异。
  - `Softmax`函数 + 交叉熵损失函数.



### 梯度下降

- **定义**：梯度下降法是一种用于最小化目标函数的优化算法。
- **原理**：
  - 它通过反复调整参数，逐步逼近目标函数的最小值点。具体来说，梯度下降法通过计算目标函数的梯度（即函数在某点的导数）来确定参数调整的方向和步长。
  - 目标函数在某点的梯度方向表示函数增长最快的方向，因此，为了找到函数的最小值，我们需要沿着梯度的反方向（即负梯度方向）调整参数。参数更新的公式为
  - 数学公式：$θ=θ−α⋅∇J(θ)$
    - *θ* 是模型的参数。
    - *α* 是学习率，控制每次更新的步长。
    - ∇*J*(*θ*) 是目标函数 *J* 关于参数 *θ* 的梯度。

- **工作步骤**：
  - **初始化参数**：选择一个初始参数 *θ*，通常为零或小的随机数。
  - **计算梯度**：计算目标函数关于当前参数的梯度 ∇*J*(*θ*)。
  - **更新参数**：根据梯度和学习率更新参数 *θ*。
  - **迭代**：重复上述步骤，直到目标函数的值收敛或达到最大迭代次数。

