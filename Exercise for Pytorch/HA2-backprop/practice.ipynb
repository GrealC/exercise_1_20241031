{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "def get_data():\n",
    "    train_data = np.genfromtxt('iris_training.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "    test_data = np.genfromtxt('iris_test.csv', delimiter=',', skip_header=1, dtype=float)\n",
    "\n",
    "    train_x = train_data[:, :-1]\n",
    "    train_y = train_data[:,-1].astype(np.int64)\n",
    "    test_x = test_data[:, :-1]\n",
    "    test_y = test_data[:,-1].astype(np.int64)\n",
    "\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失计算\n",
    "def compute_neural_net_loss(params, X, y, reg=0.0):\n",
    "    \n",
    "    # 前向传播\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    num, dim = X.shape\n",
    "\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    relu = lambda x: x * (x > 0)\n",
    "    h = relu(z1)\n",
    "\n",
    "    z2 = np.dot(h, W2) + b2\n",
    "\n",
    "    y_pred = (np.exp(z2) - np.max(z2, axis=1, keepdims=True)) / np.sum(np.exp(z2), axis=1, keepdims=True)\n",
    "\n",
    "    # 计算损失\n",
    "    loss = -np.sum(np.log(y_pred[range(num), y])) / num\n",
    "    loss += 0.5 * reg * (np.sum(W1 * W1) + np.sum(W2 * W2))\n",
    "    # 反向传播\n",
    "\n",
    "    dz2 = y_pred.copy()\n",
    "    dz2[range(num), y] -= 1 # 损失函数对z2的导数\n",
    "    dz2 /= num # 平均化，防止数值过大\n",
    "\n",
    "    dW2 = np.dot(h.T, dz2) + reg * W2 # 损失函数对W2的导数\n",
    "    db2 = np.sum(dz2, axis=0) # 损失函数对b2的导数\n",
    "    dh = np.dot(dz2, W2.T) # 损失函数对h的导数\n",
    "    dz1 = dh * (z1 > 0)\n",
    "    dW1 = np.dot(X.T, dz1) + reg * W1\n",
    "    db1 = np.sum(dz1, axis=0)\n",
    "\n",
    "    grads = {}\n",
    "    grads['W2'] = dW2\n",
    "    grads['b2'] = db2\n",
    "    grads['W1'] = dW1\n",
    "    grads['b1'] = db1\n",
    "\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "def predict(params, X):\n",
    "\n",
    "    W1, b1 = params['W1'], params['b1']\n",
    "    W2, b2 = params['W2'], params['b2']\n",
    "\n",
    "    relu = lambda x: x * (x > 0)\n",
    "\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    h = relu(z1)\n",
    "    z2 = np.dot(h, W2) + b2\n",
    "    y_pred = np.argmax(z2, axis=1)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准确率\n",
    "def acc(y, y_pred):\n",
    "    return np.mean(y == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机梯度下降\n",
    "def sgd_update(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Perform sgd update for parameters in params.\n",
    "    \"\"\"\n",
    "    for key in params:\n",
    "        params[key] += -learning_rate * grads[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train(X, y, Xtest, ytest, learning_rate=1e-3, reg=1e-5, epochs=100, batch_size=20):\n",
    "    num_train, dim = X.shape\n",
    "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "    num_iters_per_epoch = int(math.floor(1.0*num_train/batch_size))\n",
    "    \n",
    "    # In this exercise, we are going to work with a two-layer neural network\n",
    "    # first layer is a relu layer with 10 units, and second one is a softmax layer.\n",
    "    # randomly initialize parameters\n",
    "    params = {}\n",
    "    std = 0.001\n",
    "    params['W1'] = std * np.random.randn(dim, 10)\n",
    "    params['b1'] = np.zeros(10)\n",
    "    params['W2'] = std * np.random.randn(10, num_classes)\n",
    "    params['b2'] = np.zeros(num_classes)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        perm_idx = np.random.permutation(num_train) \n",
    "        '''\n",
    "        在每个 epoch 开始时，使用 np.random.permutation 函数对训练样本的索引进行随机打乱\n",
    "        '''\n",
    "        # perform mini-batch SGD update\n",
    "        for it in range(num_iters_per_epoch):\n",
    "            idx = perm_idx[it*batch_size:(it+1)*batch_size]\n",
    "            batch_x = X[idx]\n",
    "            batch_y = y[idx]\n",
    "            \n",
    "            # evaluate loss and gradient\n",
    "            loss, grads = compute_neural_net_loss(params, batch_x, batch_y, reg)\n",
    "\n",
    "            # update parameters\n",
    "            sgd_update(params, grads, learning_rate)\n",
    "            \n",
    "        # evaluate and print every 10 steps\n",
    "        if epoch % 10 == 0:\n",
    "            train_acc = acc(y, predict(params, X))\n",
    "            test_acc = acc(ytest, predict(params, Xtest))\n",
    "            print('Epoch %4d: loss = %.2f, train_acc = %.4f, test_acc = %.4f' \\\n",
    "                % (epoch, loss, train_acc, test_acc))\n",
    "    \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0: loss = 1.14, train_acc = 0.3500, test_acc = 0.2667\n",
      "Epoch   10: loss = 1.12, train_acc = 0.3500, test_acc = 0.2667\n",
      "Epoch   20: loss = 0.55, train_acc = 0.7583, test_acc = 0.6000\n",
      "Epoch   30: loss = 0.34, train_acc = 0.7917, test_acc = 0.8667\n",
      "Epoch   40: loss = 0.45, train_acc = 0.9333, test_acc = 0.9333\n",
      "Epoch   50: loss = 0.15, train_acc = 0.9667, test_acc = 0.9667\n",
      "Epoch   60: loss = 0.19, train_acc = 0.9583, test_acc = 0.9667\n",
      "Epoch   70: loss = 0.08, train_acc = 0.9500, test_acc = 0.9333\n",
      "Epoch   80: loss = 0.12, train_acc = 0.9333, test_acc = 0.9333\n",
      "Epoch   90: loss = 0.10, train_acc = 0.9917, test_acc = 0.9667\n",
      "Epoch  100: loss = 0.19, train_acc = 0.9667, test_acc = 0.9667\n",
      "Epoch  110: loss = 0.60, train_acc = 0.6917, test_acc = 0.7333\n",
      "Epoch  120: loss = 0.37, train_acc = 0.9333, test_acc = 0.9333\n",
      "Epoch  130: loss = 0.19, train_acc = 0.9750, test_acc = 0.9667\n",
      "Epoch  140: loss = 0.07, train_acc = 0.9417, test_acc = 0.9667\n",
      "Epoch  150: loss = 0.18, train_acc = 0.9083, test_acc = 0.9333\n",
      "Epoch  160: loss = 0.10, train_acc = 0.9833, test_acc = 0.9667\n",
      "Epoch  170: loss = 0.19, train_acc = 0.9833, test_acc = 0.9667\n",
      "Epoch  180: loss = 0.20, train_acc = 0.9833, test_acc = 0.9667\n",
      "Epoch  190: loss = 0.11, train_acc = 0.9833, test_acc = 0.9667\n",
      "New Samples, Class Predictions:    [1 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "max_epochs = 200\n",
    "batch_size = 20\n",
    "learning_rate = 0.1\n",
    "reg = 0.001\n",
    "\n",
    "train_x, train_y, test_x, test_y = get_data()\n",
    "params = train(train_x, train_y, test_x, test_y, learning_rate, reg, max_epochs, batch_size)\n",
    "\n",
    "# Classify two new flower samples.\n",
    "def new_samples():\n",
    "    return np.array(\n",
    "      [[6.4, 3.2, 4.5, 1.5],\n",
    "       [5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\n",
    "new_x = new_samples()\n",
    "predictions = predict(params, new_x)\n",
    "\n",
    "print(\"New Samples, Class Predictions:    {}\\n\".format(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpy38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
